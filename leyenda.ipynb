{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1897784",
   "metadata": {},
   "source": [
    "## Equipe Bordeaux\n",
    "\n",
    "L'équipe est composé d'étudiants de CESI en option **Data Science** organisé de cette manière :\n",
    "- Elise Beaumatin (Chef de Projet)\n",
    "- Axel Brosset (Data Scientist)\n",
    "- Gaylord Girer (Data Scientist)\n",
    "- Mathieu Musard (Scribe et Misogyne)\n",
    "- Xavier Labarbe (Modo Discord et Git Master)\n",
    "\n",
    "[![](https://mermaid.ink/img/pako:eNpVj7uOAjEMRX_Fcs38wBRIDINooFoKRDKFIWYnUh4jJ0EgxL-TXYEErnx9TnF9x1M0jC3-Ck0j7HodoM5CrZxNDB1T8ZRtGKBp5tCpxZUddBJT4jy83H-0VGu6uSgG1la-Wa-2lEfLBbYlkZhPtlJ7ulgW2NCR5MgDztCzeLKmlrr_mRrzyJ41tnU1fKbiskYdHlWlkuPPLZywzVJ4hmUylLm3VN_x2J7JpXqdKBxifOfHE6HJTu0?type=png)](https://mermaid.live/edit#pako:eNpVj7uOAjEMRX_Fcs38wBRIDINooFoKRDKFIWYnUh4jJ0EgxL-TXYEErnx9TnF9x1M0jC3-Ck0j7HodoM5CrZxNDB1T8ZRtGKBp5tCpxZUddBJT4jy83H-0VGu6uSgG1la-Wa-2lEfLBbYlkZhPtlJ7ulgW2NCR5MgDztCzeLKmlrr_mRrzyJ41tnU1fKbiskYdHlWlkuPPLZywzVJ4hmUylLm3VN_x2J7JpXqdKBxifOfHE6HJTu0)\n",
    "\n",
    "Cette équipe sera mise en compétition contre celle de Pau et celle de Toulouse. Elle vous montrera sa démarche et son travail pour arriver à obtenir l'Intelligence Artificielle la plus performante pour ce projet.\n",
    "\n",
    "## 1. Importation des modules et des données\n",
    "Pour créer et faire fonctionner cette IA nous aurons besoin d'importer certains modules ainsi que de la données pour entrainer et valider nos modèles de Deep Learning.\n",
    "### 1.1 Importation des modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed6b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "import glob\n",
    "from termcolor import colored\n",
    "\n",
    "yellow = '\\033[93m'\n",
    "green = '\\033[92m'\n",
    "red = '\\033[91m'\n",
    "blue = '\\033[94m'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1adb853b",
   "metadata": {},
   "source": [
    "Les principaux modules dont nous aurons besoin seront tensorflow pour créer et utiliser notre modèle, matplotlib pour afficher nos résultats et pandas pour formater et utiliser nos données.\n",
    "### 1.2 Extraction des données\n",
    "Nos données étant livré sous un format zip nous avons décider d'automatiser leur extraction et leur rangement dans deux catégories **`Photo`** et **`Other`**  qui correspondent à la classification binaire de notre livrable. Si l'image est une photo elle sera rangé dans  **`Photo`** et si l'image est un peinture, un dessin, un plan ou un texte, elle sera rangé dans **`Other`**.\n",
    "Nous ajoutons un paramètre `allow_extraction` permettant à notre fonction de ne pas s'éxécuter si l'on se souhaite pas et passe la partie extraction des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031636bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "allow_extraction = False\n",
    "datasets_path = r\"C:\\Users\\axelb\\Desktop\\Projet DS\\Datasets\"\n",
    "extracted_datasets_path = os.path.join(datasets_path,'Extracted')\n",
    "new_dataset_filename = \"Other\"\n",
    "new_dataset_path = os.path.join(extracted_datasets_path, new_dataset_filename)\n",
    "datasets_to_regroup = ['Dataset','Painting','Schematics','Sketch','Text']\n",
    "\n",
    "if allow_extraction:\n",
    "    try:\n",
    "        for dataset_filename in os.listdir(datasets_path):\n",
    "\n",
    "            dataset_path = os.path.join(datasets_path, dataset_filename)\n",
    "            dataset_zip = ZipFile(dataset_path, 'r')\n",
    "\n",
    "            if not os.path.exists(extracted_datasets_path):\n",
    "                os.makedirs(extracted_datasets_path)\n",
    "                print(f\"Extraction folder successfully created at '{extracted_datasets_path}'\")\n",
    "\n",
    "            print(f\"Starting the extraction of '{dataset_filename}' at '{extracted_datasets_path}'\")\n",
    "            dataset_zip.extractall(extracted_datasets_path)\n",
    "            print(f\"'{dataset_filename}' was successfully extracted at '{extracted_datasets_path}'\")\n",
    "\n",
    "        dataset_zip.close()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if not os.path.exists(new_dataset_path):\n",
    "        os.makedirs(new_dataset_path)\n",
    "        print(f\"New folder successfully created at '{new_dataset_path}'\")\n",
    "    for dataset in os.listdir(extracted_datasets_path):\n",
    "        if dataset in datasets_to_regroup:\n",
    "            dataset_directory = os.path.join(extracted_datasets_path, dataset)\n",
    "            print(f\"Starting the copy of '{dataset_directory}' at '{new_dataset_path}'\")\n",
    "            shutil.copytree(dataset_directory, new_dataset_path, copy_function=shutil.move, dirs_exist_ok=True)\n",
    "            print(f\"'The content of {dataset_directory}' was successfully extracted at '{new_dataset_path}'\")\n",
    "            shutil.rmtree(dataset_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9016301",
   "metadata": {},
   "source": [
    "### 1.3 Pré-traitement des données\n",
    "#### Recherche d'images corrompues et rotation des images\n",
    "Notre équipe a décidé suite à de premiers essais infructueux d'implémenter une fonction de vérification de fichiers corrompus. Et par la même occasion nous avons décider d'ajouter une fonction pour tourner ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dfa599",
   "metadata": {},
   "outputs": [],
   "source": [
    "allow_search_for_corrupted = False\n",
    "allow_rotation = False\n",
    "\n",
    "images_width=[]\n",
    "images_height=[]\n",
    "\n",
    "if allow_search_for_corrupted:\n",
    "    img_paths = glob.glob(os.path.join(r\"C:\\Users\\axelb\\Desktop\\Projet DS\\Datasets\\Extracted\",'*/*.*')) # assuming you point to the directory containing the label folders.\n",
    "\n",
    "    bad_paths = []\n",
    "\n",
    "    for image_path in img_paths:\n",
    "        try:\n",
    "            img_bytes = tf.io.read_file(image_path)\n",
    "\n",
    "            decoded_img = tf.io.decode_image(img_bytes)\n",
    "            img_height = decoded_img.shape[0]\n",
    "            img_width = decoded_img.shape[1]\n",
    "\n",
    "            if img_width < img_height and allow_rotation:\n",
    "\n",
    "                print(f\"Rotating the image at {image_path}...\")\n",
    "                decoded_img = tf.image.rot90(decoded_img)\n",
    "                img_height = decoded_img.shape[0]\n",
    "                img_width = decoded_img.shape[1]\n",
    "                \n",
    "                img = tf.keras.utils.array_to_img(decoded_img)\n",
    "                rgb_im = img.convert('RGB')\n",
    "                rgb_im.save(image_path)\n",
    "\n",
    "            images_width.append(img_width)\n",
    "            images_height.append(img_height)    \n",
    "            \n",
    "        except tf.errors.InvalidArgumentError as e:\n",
    "          print(f\"Found bad path {image_path}...{e}\")\n",
    "          bad_paths.append(image_path)\n",
    "            \n",
    "    for bad_path in bad_paths:\n",
    "        print(f\"WARNING : Dropping the file at {bad_path} because it can't be open...\")\n",
    "        os.remove(bad_path)\n",
    "        \n",
    "print(f\"The median of width is {np.median(images_width)}\")\n",
    "print(f\"The median of height is {np.median(images_height)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6762183b",
   "metadata": {},
   "source": [
    "### 1.4 Importation des données\n",
    "Une fois nos données trier dans des catégories distinctes, les images corrompues retirées et des images retournées ajoutées, nous pouvons importer nos images dans des sets d'images utilisable par **Tensorflow**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce632906",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    extracted_datasets_path,\n",
    "    validation_split=0.2,\n",
    "    seed=10,\n",
    "    image_size = (224, 224),\n",
    "    batch_size=64,\n",
    "    subset = \"both\"\n",
    ")\n",
    "\n",
    "print(f\"\\nThe dataset is composed of these classes : {train_set.class_names}\")\n",
    "\n",
    "for images, labels in train_set:\n",
    "    print(f\"\\nThe dataset is composed of these classes : {images.shape}\")\n",
    "    print(f\"The dataset is composed of these labels : {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "097fa6e7",
   "metadata": {},
   "source": [
    "Nous pouvons désormais afficher des images comme par exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe96959",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "for images, labels in train_set.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i+1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(train_set.class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "db3f533099ad21a3061e7aee54228e0f82cb172edfed5de726b90a05601ed36a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
